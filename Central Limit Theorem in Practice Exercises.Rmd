---
title: "R Notebook"
output: html_notebook
---

1. The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let’s try a simple example related to gambling.

Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies.

We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). 

```{r}
# Set the seed to 1 for reproducibility
set.seed(1)

# Define the parameters
n <- 100      # Number of dice rolls
B <- 10000    # Number of simulations
p <- 1/6      # Probability of getting a 6 on a single die roll

# Simulate rolling n dice 10,000 times and calculate z-values
z_values <- replicate(B, {
  x <- sample(1:6, n, replace = TRUE)
  z <- (mean(x == 6) - p) / sqrt(p * (1 - p) / n)
  z
})

# Calculate the proportion of times |z| > 2
prop_large_z <- mean(abs(z_values) > 2)

# Print the result
cat("Proportion of times |z| > 2:", prop_large_z, "\n")

```
2. For the last simulation you can make a qqplot to confirm the normal approximation. Now, the CLT is an asympototic result, meaning it is closer and closer to being a perfect approximation as the sample size increases. In practice, however, we need to decide if it is appropriate for actual sample sizes. Is 10 enough? 15? 30?

In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to “kick in”.

Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best?
```{r}
# Set the seed for reproducibility
set.seed(1)

# Define parameters
B <- 10000   # Number of simulations
n_values <- c(10, 15, 30)  # Different sample sizes
p_values <- c(0.1, 0.3, 0.5)  # Different success probabilities

# Function to simulate and check normal approximation
simulate_and_check <- function(n, p) {
  z_values <- replicate(B, {
    x <- sample(1:6, n, replace = TRUE)
    z <- (mean(x == 6) - p) / sqrt(p * (1 - p) / n)
    z
  })
  
  qqnorm(z_values, main = paste("n =", n, ", p =", p))
  qqline(z_values, col = 2)
}

# Run simulations for different combinations of n and p
for (n in n_values) {
  for (p in p_values) {
    simulate_and_check(n, p)
  }
}

```
```{r}
# Run simulations for the specified values
simulate_and_check(5, 0.5)  # A) p=0.5 and n=5
simulate_and_check(30, 0.5)  # B) p=0.5 and n=30
simulate_and_check(30, 0.01)  # C) p=0.01 and n=30
simulate_and_check(100, 0.01)  # D) p=0.01 and n=100
```
```{r}

# Extracting the bodyweight measurements for the control diet (chow)
X <- filter(dat, Diet == "chow") %>% select(Bodyweight) %>% unlist()

# Calculating the sample average (mean) for X
sample_average_X <- mean(X)

# Displaying the result
print(paste("Sample Average (X̄) for the control diet population:", round(sample_average_X, 2)))


```

















